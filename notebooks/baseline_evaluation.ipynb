{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3bc98ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\monish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\monish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries - Python 3.8\n",
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import sklearn.preprocessing as pp\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Concatenate, Input, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import one_hot\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "join = os.path.join\n",
    "\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c6c7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries - Python 3.6\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "join = os.path.join\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d2dc475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284125b1",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bfb61888",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6696da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "\n",
    "video1_csv = \"baseline1_final.csv\"\n",
    "df_video1 = pd.read_csv(join(root_path, video1_csv), sep='\\t')\n",
    "\n",
    "channel_csv = \"baseline_final_channels.csv\"\n",
    "df_channel = pd.read_csv(join(root_path, channel_csv), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45c250e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2120, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it happened outside waco texas a heavily armed...</td>\n",
       "      <td>the shadow of waco  retro report  the new york...</td>\n",
       "      <td>0</td>\n",
       "      <td>hOW9AjskoOo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thanks for coming its nice to see a good turno...</td>\n",
       "      <td>former abortionist dr levatino destroys procho...</td>\n",
       "      <td>0</td>\n",
       "      <td>dIRcw45n9RU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tonight  i donald john trump do solemnly swear...</td>\n",
       "      <td>trumps road to the white house full film  fron...</td>\n",
       "      <td>0</td>\n",
       "      <td>SMwXKl0odq8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this week on buzzfeed unsolved we discuss the...</td>\n",
       "      <td>the strange disappearance of db cooper</td>\n",
       "      <td>0</td>\n",
       "      <td>oHSehKtDyoI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im mason noise im 22 and im from birmingham wh...</td>\n",
       "      <td>shockingly offensive auditions have simon cowe...</td>\n",
       "      <td>0</td>\n",
       "      <td>N9COy7O7K-U</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caption  \\\n",
       "0  it happened outside waco texas a heavily armed...   \n",
       "1  thanks for coming its nice to see a good turno...   \n",
       "2  tonight  i donald john trump do solemnly swear...   \n",
       "3   this week on buzzfeed unsolved we discuss the...   \n",
       "4  im mason noise im 22 and im from birmingham wh...   \n",
       "\n",
       "                                               title  label     video_id  \n",
       "0  the shadow of waco  retro report  the new york...      0  hOW9AjskoOo  \n",
       "1  former abortionist dr levatino destroys procho...      0  dIRcw45n9RU  \n",
       "2  trumps road to the white house full film  fron...      0  SMwXKl0odq8  \n",
       "3             the strange disappearance of db cooper      0  oHSehKtDyoI  \n",
       "4  shockingly offensive auditions have simon cowe...      0  N9COy7O7K-U  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_video1.shape)\n",
    "df_video1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6981ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCqnbDFdCpuN8CMEg0VuEBqA</td>\n",
       "      <td>hOW9AjskoOo,uJ44spUo8Uk,-O_DMyHdq_M,U_hbIPJuia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCfkzsfj7Go1Q_kRFZmJptsw</td>\n",
       "      <td>dIRcw45n9RU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UC3ScyryU9Oy9Wse3a8OAmYQ</td>\n",
       "      <td>SMwXKl0odq8,AW0gsP3EgDI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCKijjvu6bN1c-ZHVwR7-5WA</td>\n",
       "      <td>oHSehKtDyoI,lDeFSOUHdH4,cDZweMXXY6Y,p2EUZ-gwe6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UC6my_lD3kBECBifeq0n2mdg</td>\n",
       "      <td>N9COy7O7K-U,DHwpwD-ae7I,74fTHh6jB5Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id                                          video_ids\n",
       "0  UCqnbDFdCpuN8CMEg0VuEBqA  hOW9AjskoOo,uJ44spUo8Uk,-O_DMyHdq_M,U_hbIPJuia...\n",
       "1  UCfkzsfj7Go1Q_kRFZmJptsw                                        dIRcw45n9RU\n",
       "2  UC3ScyryU9Oy9Wse3a8OAmYQ                            SMwXKl0odq8,AW0gsP3EgDI\n",
       "3  UCKijjvu6bN1c-ZHVwR7-5WA  oHSehKtDyoI,lDeFSOUHdH4,cDZweMXXY6Y,p2EUZ-gwe6...\n",
       "4  UC6my_lD3kBECBifeq0n2mdg                N9COy7O7K-U,DHwpwD-ae7I,74fTHh6jB5Q"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_channel.shape)\n",
    "df_channel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c39874ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARDCODED limits per topic for baseline1 dataset\n",
    "TOPIC_LIMITS = [0, 430, 901, 1214, 1530, 2120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5aefebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions1_list = df_video1[\"caption\"].to_list()\n",
    "video1_list = df_video1[\"video_id\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92f4a5",
   "metadata": {},
   "source": [
    "### Captions -> word2vec, extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c1304fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../final\\\\GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24904/1774952539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load word2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mword2vec_300\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"GoogleNews-vectors-negative300.bin\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mword2vec_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1545\u001b[0m         \"\"\"\n\u001b[0;32m   1546\u001b[0m         \u001b[1;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1547\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m             limit=limit, datatype=datatype)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    303\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../final\\\\GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "# Load word2vec\n",
    "word2vec_300 = \"GoogleNews-vectors-negative300.bin\"\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(join(root_path, word2vec_300), binary = True)\n",
    "word2vec_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe751bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    return np.mean(word2vec_model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our earlier preprocessing was done when we were dealing only with word vectors\n",
    "# Here, we need each document to remain a document \n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word.isalpha()] \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will help us drop documents that have no word vectors in word2vec\n",
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.vocab for word in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out documents\n",
    "def filter_docs(corpus, texts, video_ids, condition_on_doc):\n",
    "    \"\"\"\n",
    "    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "\n",
    "    ret_texts, ret_videos = [], []\n",
    "    if texts is not None:\n",
    "        for (text, doc, video_id) in zip(texts, corpus, video_ids):\n",
    "            if condition_on_doc(doc):\n",
    "                ret_texts.append(text)\n",
    "                ret_videos.append(video_id)\n",
    "\n",
    "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
    "\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, ret_texts, ret_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def captions_to_word2vec(caption_list, video_list, model):\n",
    "    # Preprocess the corpus\n",
    "    corpus = [preprocess(title) for title in caption_list]\n",
    "\n",
    "    # Remove docs that don't include any words in W2V's vocab\n",
    "    corpus, titles_list, video_list = filter_docs(corpus, caption_list, video_list, lambda doc: has_vector_representation(model, doc))\n",
    "\n",
    "    # Filter out any empty docs\n",
    "    corpus, titles_list, video_list = filter_docs(corpus, caption_list, video_list, lambda doc: (len(doc) != 0))\n",
    "    x = []\n",
    "    for doc in corpus: # append the vector for each document\n",
    "        x.append(document_vector(model, doc))\n",
    "\n",
    "    X = np.array(x) # list to array\n",
    "    \n",
    "    return X, video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccda93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions1_X, cap_w2v_videos = captions_to_word2vec(captions1_list, video1_list, word2vec_model)\n",
    "captions1_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77797ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(join(root_path, 'baseline1_word2vec_300.npy'), 'wb') as f:\n",
    "    np.save(f, captions1_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761dfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "assert os.path.isfile(join(root_path, 'baseline1_word2vec_300.npy'))\n",
    "captions1_X = np.load(join(root_path, 'baseline1_word2vec_300.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513815b",
   "metadata": {},
   "source": [
    "### Captions -> fastText\n",
    "Source: https://github.com/kostantinos-papadamou/pseudoscience-paper\n",
    "Use Python 3.6 to run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "82cd6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data, save to file\n",
    "with open(join(root_path, 'fasttext_captions.txt'), 'w') as f:\n",
    "    for item in captions1_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef16199e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "../final\\wiki-news-300d-1M.vec cannot be opened for loading!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24904/2731007022.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fine tune fasttext model, save to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfasttext_models_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'fasttext_model_finetuned.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m ft_model = fasttext.train_unsupervised(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fasttext_captions.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpretrainedVectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wiki-news-300d-1M.vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\fasttext\\FastText.py\u001b[0m in \u001b[0;36mtrain_unsupervised\u001b[1;34m(*kargs, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanually_set_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[0mft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m     \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m     \u001b[0mft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ../final\\wiki-news-300d-1M.vec cannot be opened for loading!"
     ]
    }
   ],
   "source": [
    "# Fine tune fasttext model, save to file\n",
    "fasttext_models_filename = 'fasttext_model_finetuned.bin'\n",
    "ft_model = fasttext.train_unsupervised(\n",
    "    input=join(root_path, 'fasttext_captions.txt'),\n",
    "    pretrainedVectors=join(root_path, 'wiki-news-300d-1M.vec'),\n",
    "    dim=300,\n",
    "    minn=2,\n",
    "    maxn=5,\n",
    "    verbose=2)\n",
    "ft_model.save_model(join(root_path, fasttext_models_filename))\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8ba9ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2120, 300)\n"
     ]
    }
   ],
   "source": [
    "# Get caption features\n",
    "ft_model = fasttext.load_model(join(root_path, \"fasttext_model_finetuned-002.bin\"))\n",
    "ft_features = []\n",
    "for caption in captions1_list:\n",
    "    ft_features += [ft_model.get_sentence_vector(text=caption)]\n",
    "ft_features = np.array(ft_features)\n",
    "print(ft_features.shape)\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a79da6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to file\n",
    "# with open(join(root_path, 'baseline1_fasttext_300.npy'), 'wb') as f:\n",
    "#     np.save(f, ft_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc798ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "assert os.path.isfile(join(root_path, 'baseline1_fasttext_300.npy'))\n",
    "ft_features = np.load(join(root_path, 'baseline1_fasttext_300.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f8765e",
   "metadata": {},
   "source": [
    "### Train deep learning model on fastText, extract embeddings\n",
    "Source: https://github.com/kostantinos-papadamou/pseudoscience-paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify GPU environment\n",
    "gpu_training = True\n",
    "if gpu_training:\n",
    "    # Train on GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "else:\n",
    "    # Train on CPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32559d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve labels\n",
    "ft_labels = df_video1[\"label\"].to_numpy()\n",
    "ft_labels[ft_labels == -1] = 0  # make two class\n",
    "ft_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da9349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " fully_connected_1 (Dense)   (None, 256)               77056     \n",
      "                                                                 \n",
      " dropout_layer_1 (Dropout)   (None, 256)               0         \n",
      "                                                                 \n",
      " fully_connected_2 (Dense)   (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_layer_2 (Dropout)   (None, 128)               0         \n",
      "                                                                 \n",
      " fully_connected_3 (Dense)   (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_layer_3 (Dropout)   (None, 64)                0         \n",
      "                                                                 \n",
      " fully_connected_4 (Dense)   (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_layer_4 (Dropout)   (None, 32)                0         \n",
      "                                                                 \n",
      " classification_layer (Dense  (None, 2)                66        \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,354\n",
      "Trainable params: 120,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ericm/anaconda3/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "class PsuedoscienceDeepLearningModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize hyperparameters\n",
    "        self.embedding_dim = 300\n",
    "        self.dropout = 0.5\n",
    "        self.learning_rate = 1e-3\n",
    "        self.val_split_size = 0.2\n",
    "        self.num_epochs = 100\n",
    "        self.num_folds = 10  # for k-fold cross validation\n",
    "        self.batch_size = 20\n",
    "        self.shuffle_train_set = True\n",
    "        self.oversampling = True\n",
    "        self.num_classes = 2\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        seq = Sequential()\n",
    "        seq.add(Dense(units=256, activation='relu', name='fully_connected_1', input_shape=(self.embedding_dim,)))\n",
    "        seq.add(Dropout(rate=self.dropout, name='dropout_layer_1'))\n",
    "        seq.add(Dense(units=128, activation='relu', name='fully_connected_2'))\n",
    "        seq.add(Dropout(rate=self.dropout, name='dropout_layer_2'))\n",
    "        seq.add(Dense(units=64, activation='relu', name='fully_connected_3'))\n",
    "        seq.add(Dropout(rate=self.dropout, name='dropout_layer_3'))\n",
    "        seq.add(Dense(units=32, activation='relu', name='fully_connected_4'))\n",
    "        seq.add(Dropout(rate=self.dropout, name='dropout_layer_4'))\n",
    "        seq.add(Dense(units=self.num_classes, activation='softmax', name='classification_layer'))\n",
    "        seq.compile(loss=BinaryCrossentropy(from_logits=False),\n",
    "                    optimizer=Adam(lr=self.learning_rate),\n",
    "                    metrics=[F1Score(num_classes=2)])\n",
    "        return seq\n",
    "    \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "model = PsuedoscienceDeepLearningModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc79476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               verbose=2,\n",
    "                               mode='auto',\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "796ea3b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ft_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24904/1568781314.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/n---Training the Model with {} videos.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0;32m      5\u001b[0m     ft_features, ft_labels, test_size=model.val_split_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ft_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print('/n---Training the Model with {} videos.'.format(ft_features.shape[0]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ft_features, ft_labels, test_size=model.val_split_size)\n",
    "\n",
    "# Oversampling\n",
    "if model.oversampling:\n",
    "    smote = SMOTE(sampling_strategy='not majority')\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    print('--- [AFTER OVER-SAMPLING] TRAIN: %d' % (y_train.shape[0]))\n",
    "\n",
    "# Convert labels to 1-hot\n",
    "y_train = one_hot(y_train, model.num_classes)\n",
    "y_test = one_hot(y_test, model.num_classes)\n",
    "\n",
    "# Train the model\n",
    "model_train_input = [X_train]\n",
    "model_val_input = [X_test]\n",
    "m = model.get_model()\n",
    "m.fit(model_train_input,\n",
    "      y_train,\n",
    "      epochs=model.num_epochs,\n",
    "      batch_size=model.batch_size,\n",
    "      validation_data=[model_val_input, y_test],\n",
    "      shuffle=model.shuffle_train_set,\n",
    "      verbose=1,\n",
    "      callbacks=[early_stopping])\n",
    "\n",
    "# Save trained model\n",
    "m.save(join(root_path, 'nn.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c372305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = load_model(join(root_path, \"nn_75.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c125a4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve layer outputs\n",
    "layer_name = 'fully_connected_4'\n",
    "intermediate_model = Model(inputs=best_model.input,\n",
    "                          outputs=best_model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_model.predict(ft_features)\n",
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d911e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "97559605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "assert os.path.isfile(join(root_path, 'baseline1_nn_32.npy'))\n",
    "intermediate_output = np.load(join(root_path, 'baseline1_nn_32.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9f1f9",
   "metadata": {},
   "source": [
    "### Load embeddings from our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4d9aba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_novel = np.load(join(root_path, 'bi_embedding.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "758244e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert from ragged tensor to normal numpy array\n",
    "# X_novel = np.zeros((ce.shape[0], ce[0][0].shape[0]))\n",
    "# for i in range(ce.shape[0]):\n",
    "#     X_novel[i, :] = ce[i][0]\n",
    "# X_novel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a5fef",
   "metadata": {},
   "source": [
    "### Normalize CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8de2fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_novel.shape[0]):\n",
    "    X_novel[i] = X_novel[i] / np.linalg.norm(X_novel[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "41e52425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00104776, -0.00723376,  0.01471311],\n",
       "       [-0.00116783, -0.00725456,  0.01445016],\n",
       "       [-0.00166335, -0.00807987,  0.01418412]], dtype=float32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_novel[:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ee9c9",
   "metadata": {},
   "source": [
    "### Compute similarity between channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "75e0e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarities(mat):\n",
    "    col_normed_mat = pp.normalize(coo_matrix(mat.T).tocsc(), axis=0)\n",
    "    return col_normed_mat.T * col_normed_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3f10be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ground_truth_diff_for_channels(baseline_final, baseline_channels):\n",
    "    \n",
    "    num_channels = len(baseline_channels)\n",
    "    channel_ground_truth_proportion = np.empty(num_channels, dtype='float32')\n",
    "\n",
    "    for index, row in baseline_channels.iterrows():\n",
    "        videos = row['video_ids'].split(',')\n",
    "\n",
    "        channel_videos = baseline_final[baseline_final[\"video_id\"].isin(videos)]\n",
    "        proportion = np.mean(channel_videos[\"label\"].to_numpy())\n",
    "        channel_ground_truth_proportion[index] = proportion\n",
    "    \n",
    "    return channel_ground_truth_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cd97424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ground_truth_diff(p):\n",
    "    return np.abs(p[:, np.newaxis] - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4614c1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884, 884)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute ground truth proportion differences for channels\n",
    "misinfo_proportions = compute_ground_truth_diff_for_channels(df_video1, df_channel)\n",
    "proportion_diffs = compute_ground_truth_diff(misinfo_proportions)\n",
    "proportion_diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ab3a91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_embeddings(df_video, df_channel, video_embeddings):\n",
    "    \n",
    "    num_channels = len(df_channel)\n",
    "    embedding_size = video_embeddings.shape[1]\n",
    "    channel_embeddings = np.empty((num_channels, embedding_size), dtype='float32')\n",
    "    \n",
    "    for index, row in df_channel.iterrows():\n",
    "        videos = row['video_ids'].split(',')\n",
    "        \n",
    "        video_i = df_video.index[df_video[\"video_id\"].isin(videos)]\n",
    "        channel_embeddings[index, :] = np.mean(video_embeddings[video_i, :], axis=0)\n",
    "    \n",
    "    return channel_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0782351f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((884, 300), (884, 32), (884, 768))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute channel embeddings for baselines\n",
    "word2vec_channel_emb = compute_channel_embeddings(df_video1, df_channel, captions1_X)\n",
    "nn_channel_emb = compute_channel_embeddings(df_video1, df_channel, intermediate_output)\n",
    "novel_channel_emb = compute_channel_embeddings(df_video1, df_channel, X_novel)\n",
    "word2vec_channel_emb.shape, nn_channel_emb.shape, novel_channel_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2be9d990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((884, 884), (884, 884), (884, 884))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cosine similarities\n",
    "word2vec_channel_sim = cosine_similarities(word2vec_channel_emb).toarray()\n",
    "nn_channel_sim = cosine_similarities(nn_channel_emb).toarray()\n",
    "novel_channel_sim = cosine_similarities(novel_channel_emb).toarray()\n",
    "word2vec_channel_sim.shape, nn_channel_sim.shape, novel_channel_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b2e349cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09198472 1.0\n",
      "0.0 1.0\n",
      "0.94883376 1.0\n"
     ]
    }
   ],
   "source": [
    "# All cosine similarities must be between 0 and 1\n",
    "\n",
    "word2vec_channel_sim[word2vec_channel_sim > 1.0] = 1.0\n",
    "word2vec_channel_sim[word2vec_channel_sim < 0.0] = 0.0\n",
    "nn_channel_sim[nn_channel_sim > 1.0] = 1.0\n",
    "nn_channel_sim[nn_channel_sim < 0.0] = 0.0\n",
    "novel_channel_sim[novel_channel_sim > 1.0] = 1.0\n",
    "novel_channel_sim[novel_channel_sim < 0.0] = 0.0\n",
    "\n",
    "print(word2vec_channel_sim.min(), word2vec_channel_sim.max())\n",
    "print(nn_channel_sim.min(), nn_channel_sim.max())\n",
    "print(novel_channel_sim.min(), novel_channel_sim.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a0f8d9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 37, 125)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter channels according to number of videos\n",
    "# A minimum of 3 is required for fair comparison\n",
    "filtered_indices = []\n",
    "filtered_misinfo_indices = []\n",
    "filtered_nonmisinfo_indices = []\n",
    "for index, row in df_channel.iterrows():\n",
    "    videos = row['video_ids'].split(',')\n",
    "    if len(videos) >= 3:\n",
    "        filtered_indices += [index]\n",
    "        \n",
    "        channel_videos = df_video1[df_video1[\"video_id\"].isin(videos)]\n",
    "        if (channel_videos['label'] == 1).sum() > 0:\n",
    "            filtered_misinfo_indices += [index]\n",
    "        else:\n",
    "            filtered_nonmisinfo_indices += [index]\n",
    "\n",
    "len(filtered_indices), len(filtered_misinfo_indices), len(filtered_nonmisinfo_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f346e0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((162, 162), (37, 37), (125, 125))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter proportion diff and similarity matrices accordingly\n",
    "pd_all = proportion_diffs[filtered_indices, :][:, filtered_indices]\n",
    "pd_misinfo = proportion_diffs[filtered_misinfo_indices, :][:, filtered_misinfo_indices]\n",
    "pd_nonmisinfo = proportion_diffs[filtered_nonmisinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "w2v_sim_all = word2vec_channel_sim[filtered_indices, :][:, filtered_indices]\n",
    "w2v_sim_misinfo = word2vec_channel_sim[filtered_misinfo_indices, :][:, filtered_misinfo_indices]\n",
    "w2v_sim_nonmisinfo = word2vec_channel_sim[filtered_nonmisinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "nn_sim_all = nn_channel_sim[filtered_indices, :][:, filtered_indices]\n",
    "nn_sim_misinfo = nn_channel_sim[filtered_misinfo_indices, :][:, filtered_misinfo_indices]\n",
    "nn_sim_nonmisinfo = nn_channel_sim[filtered_nonmisinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "novel_sim_all = novel_channel_sim[filtered_indices, :][:, filtered_indices]\n",
    "novel_sim_misinfo = novel_channel_sim[filtered_misinfo_indices, :][:, filtered_misinfo_indices]\n",
    "novel_sim_nonmisinfo = novel_channel_sim[filtered_nonmisinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "pd_all.shape, pd_misinfo.shape, pd_nonmisinfo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a6c22d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13041,), (666,), (7750,))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract upper diagonals as flat matrix\n",
    "\n",
    "ind_all = np.triu_indices(len(filtered_indices), k=1)\n",
    "ind_misinfo = np.triu_indices(len(filtered_misinfo_indices), k=1)\n",
    "ind_nonmisinfo = np.triu_indices(len(filtered_nonmisinfo_indices), k=1)\n",
    "\n",
    "pd_all_ud = pd_all[ind_all]\n",
    "pd_misinfo_ud = pd_misinfo[ind_misinfo]\n",
    "pd_nonmisinfo_ud = pd_nonmisinfo[ind_nonmisinfo]\n",
    "\n",
    "w2v_sim_all_ud = w2v_sim_all[ind_all]\n",
    "w2v_sim_misinfo_ud = w2v_sim_misinfo[ind_misinfo]\n",
    "w2v_sim_nonmisinfo_ud = w2v_sim_nonmisinfo[ind_nonmisinfo]\n",
    "\n",
    "nn_sim_all_ud = nn_sim_all[ind_all]\n",
    "nn_sim_misinfo_ud = nn_sim_misinfo[ind_misinfo]\n",
    "nn_sim_nonmisinfo_ud = nn_sim_nonmisinfo[ind_nonmisinfo]\n",
    "\n",
    "novel_sim_all_ud = novel_sim_all[ind_all]\n",
    "novel_sim_misinfo_ud = novel_sim_misinfo[ind_misinfo]\n",
    "novel_sim_nonmisinfo_ud = novel_sim_nonmisinfo[ind_nonmisinfo]\n",
    "\n",
    "pd_all_ud.shape, pd_misinfo_ud.shape, pd_nonmisinfo_ud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "16c5c3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4625,), (4625,), (4625,), (4625,))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract cosine similarities of (misinfo, nonmisinfo) pairs\n",
    "# This needs to be done separately as it won't be a square matrix\n",
    "\n",
    "pd_both = proportion_diffs[filtered_misinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "w2v_sim_both = word2vec_channel_sim[filtered_misinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "nn_sim_both = nn_channel_sim[filtered_misinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "novel_sim_both = novel_channel_sim[filtered_misinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "\n",
    "# Naming convention: _ud to be similar to other variables\n",
    "# No actual upper triangular calculation here\n",
    "pd_both_ud = pd_both.flatten()\n",
    "w2v_sim_both_ud = w2v_sim_both.flatten()\n",
    "nn_sim_both_ud = nn_sim_both.flatten()\n",
    "novel_sim_both_ud = novel_sim_both.flatten()\n",
    "\n",
    "pd_both_ud.shape, w2v_sim_both_ud.shape, nn_sim_both_ud.shape, novel_sim_both_ud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "004423b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_analysis_by_range(proportion_diffs_ud, sim_ud):\n",
    "\n",
    "    ranges = [(0.0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.01)]\n",
    "    for begin, end in ranges:\n",
    "\n",
    "        ind = np.argwhere((proportion_diffs_ud >= begin) & (proportion_diffs_ud < end))\n",
    "        \n",
    "        if ind.shape[0] > 0:\n",
    "            #proportion_diffs_i = proportion_diffs_ud[ind]\n",
    "            sim_i = sim_ud[ind]\n",
    "\n",
    "            print(\"For range\", begin, \"to\", end, \" - \", sim_i.shape[0], \"pairs\")\n",
    "            #print(\"25th percentile =\", np.percentile(sim_i, 25))\n",
    "            #print(\"50th percentile =\", np.percentile(sim_i, 50))\n",
    "            #print(\"75th percentile =\", np.percentile(sim_i, 75))\n",
    "            print(\"{0},{1},{2}\".format(np.percentile(sim_i, 25), np.percentile(sim_i, 50), np.percentile(sim_i, 75)))\n",
    "        else:\n",
    "            print(\"No pairs for range\", begin, \"to\", end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4a01d",
   "metadata": {},
   "source": [
    "#### Results: any label to any label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d4a8bc29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  4591 pairs\n",
      "0.9349738955497742,0.9611316323280334,0.9763515889644623\n",
      "For range 0.2 to 0.4  -  3367 pairs\n",
      "0.9307077527046204,0.9571183323860168,0.973841518163681\n",
      "For range 0.4 to 0.6  -  1536 pairs\n",
      "0.9287628084421158,0.9563179314136505,0.9732555747032166\n",
      "For range 0.6 to 0.8  -  1472 pairs\n",
      "0.9185249358415604,0.9571459591388702,0.973799392580986\n",
      "For range 0.8 to 1.01  -  1284 pairs\n",
      "0.914280965924263,0.9543503224849701,0.9731230735778809\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_all_ud, w2v_sim_all_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1e9c0ddb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  4591 pairs\n",
      "0.9878326952457428,0.998088538646698,0.9996699690818787\n",
      "For range 0.2 to 0.4  -  3367 pairs\n",
      "0.9718308746814728,0.9965531826019287,0.9993865191936493\n",
      "For range 0.4 to 0.6  -  1536 pairs\n",
      "0.7102301567792892,0.9849448800086975,0.9980695396661758\n",
      "For range 0.6 to 0.8  -  1472 pairs\n",
      "0.8472627848386765,0.9765549600124359,0.9961894154548645\n",
      "For range 0.8 to 1.01  -  1284 pairs\n",
      "0.35030514746904373,0.9182875454425812,0.9919017404317856\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_all_ud, nn_sim_all_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5e579aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  4591 pairs\n",
      "0.9991021752357483,0.9997085332870483,0.9999262392520905\n",
      "For range 0.2 to 0.4  -  3367 pairs\n",
      "0.9988580942153931,0.9996315240859985,0.9999039173126221\n",
      "For range 0.4 to 0.6  -  1536 pairs\n",
      "0.998951867222786,0.999666690826416,0.9999134689569473\n",
      "For range 0.6 to 0.8  -  1472 pairs\n",
      "0.9990434050559998,0.9996761083602905,0.9999018460512161\n",
      "For range 0.8 to 1.01  -  1284 pairs\n",
      "0.998866617679596,0.9995671510696411,0.9998558312654495\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_all_ud, novel_sim_all_ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a3992",
   "metadata": {},
   "source": [
    "#### Results: misinfo to misinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "801160b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  147 pairs\n",
      "0.952852189540863,0.9694886803627014,0.9803795516490936\n",
      "For range 0.2 to 0.4  -  155 pairs\n",
      "0.9491956532001495,0.9687919020652771,0.9776621162891388\n",
      "For range 0.4 to 0.6  -  122 pairs\n",
      "0.9475528746843338,0.9666178226470947,0.9770339131355286\n",
      "For range 0.6 to 0.8  -  107 pairs\n",
      "0.9530268609523773,0.97138911485672,0.9801283180713654\n",
      "For range 0.8 to 1.01  -  79 pairs\n",
      "0.9538569748401642,0.9685057401657104,0.9792028069496155\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_misinfo_ud, w2v_sim_misinfo_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "39c1ba40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  147 pairs\n",
      "0.6253405511379242,0.9908631443977356,0.9980990290641785\n",
      "For range 0.2 to 0.4  -  155 pairs\n",
      "0.4010315388441086,0.992784857749939,0.998765230178833\n",
      "For range 0.4 to 0.6  -  122 pairs\n",
      "0.16523493826389313,0.8675801455974579,0.9913467019796371\n",
      "For range 0.6 to 0.8  -  107 pairs\n",
      "0.04899721406400204,0.2615395486354828,0.9801926910877228\n",
      "For range 0.8 to 1.01  -  79 pairs\n",
      "0.06611417979001999,0.23036248981952667,0.9115776717662811\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_misinfo_ud, nn_sim_misinfo_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "530dc646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  147 pairs\n",
      "0.9987797737121582,0.999619722366333,0.999900609254837\n",
      "For range 0.2 to 0.4  -  155 pairs\n",
      "0.999045729637146,0.9997037053108215,0.9999285340309143\n",
      "For range 0.4 to 0.6  -  122 pairs\n",
      "0.998695507645607,0.9996688961982727,0.9998880922794342\n",
      "For range 0.6 to 0.8  -  107 pairs\n",
      "0.9991702735424042,0.999756395816803,0.9999009370803833\n",
      "For range 0.8 to 1.01  -  79 pairs\n",
      "0.9988675713539124,0.9996290802955627,0.9998592138290405\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_misinfo_ud, novel_sim_misinfo_ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89d41f",
   "metadata": {},
   "source": [
    "#### Results: nonmisinfo to nonmisinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1a9e0b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  3586 pairs\n",
      "0.9310038834810257,0.958113044500351,0.974911093711853\n",
      "For range 0.2 to 0.4  -  2244 pairs\n",
      "0.9262921959161758,0.9543962776660919,0.9727998226881027\n",
      "For range 0.4 to 0.6  -  833 pairs\n",
      "0.9156664609909058,0.9499412775039673,0.9699505567550659\n",
      "For range 0.6 to 0.8  -  616 pairs\n",
      "0.8791230022907257,0.9391011297702789,0.9709326922893524\n",
      "For range 0.8 to 1.01  -  471 pairs\n",
      "0.8901519477367401,0.9309831857681274,0.9677489399909973\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_nonmisinfo_ud, w2v_sim_nonmisinfo_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5e9382b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  3586 pairs\n",
      "0.9911077618598938,0.9983079731464386,0.9997662007808685\n",
      "For range 0.2 to 0.4  -  2244 pairs\n",
      "0.9747819006443024,0.9971319437026978,0.9995608478784561\n",
      "For range 0.4 to 0.6  -  833 pairs\n",
      "0.9731502532958984,0.9849562644958496,0.9980689883232117\n",
      "For range 0.6 to 0.8  -  616 pairs\n",
      "0.9326130598783493,0.984435111284256,0.9973897933959961\n",
      "For range 0.8 to 1.01  -  471 pairs\n",
      "0.8678244948387146,0.9774820804595947,0.9961245954036713\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_nonmisinfo_ud, nn_sim_nonmisinfo_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "31816cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  3586 pairs\n",
      "0.99917933344841,0.999722808599472,0.9999313354492188\n",
      "For range 0.2 to 0.4  -  2244 pairs\n",
      "0.9988379031419754,0.9996274709701538,0.9999020099639893\n",
      "For range 0.4 to 0.6  -  833 pairs\n",
      "0.9989073872566223,0.9996014833450317,0.9999085664749146\n",
      "For range 0.6 to 0.8  -  616 pairs\n",
      "0.9991013556718826,0.9996712505817413,0.9998935163021088\n",
      "For range 0.8 to 1.01  -  471 pairs\n",
      "0.9988528788089752,0.9995086193084717,0.999803364276886\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_nonmisinfo_ud, novel_sim_nonmisinfo_ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1d0be",
   "metadata": {},
   "source": [
    "#### Results: misinfo to nonmisinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "416668c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  858 pairs\n",
      "0.9487195760011673,0.9682484269142151,0.9805682450532913\n",
      "For range 0.2 to 0.4  -  968 pairs\n",
      "0.9373989254236221,0.9595908224582672,0.9747491180896759\n",
      "For range 0.4 to 0.6  -  581 pairs\n",
      "0.9377700090408325,0.9625737071037292,0.9767392873764038\n",
      "For range 0.6 to 0.8  -  749 pairs\n",
      "0.9349206686019897,0.9612652063369751,0.9745776653289795\n",
      "For range 0.8 to 1.01  -  734 pairs\n",
      "0.9342801719903946,0.9597935974597931,0.9746307581663132\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_both_ud, w2v_sim_both_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "728f3611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  858 pairs\n",
      "0.9870851635932922,0.9972964525222778,0.999194785952568\n",
      "For range 0.2 to 0.4  -  968 pairs\n",
      "0.9642482995986938,0.9960271716117859,0.9990866184234619\n",
      "For range 0.4 to 0.6  -  581 pairs\n",
      "0.5341629981994629,0.9900873899459839,0.9986177086830139\n",
      "For range 0.6 to 0.8  -  749 pairs\n",
      "0.44558992981910706,0.9713408350944519,0.9961678981781006\n",
      "For range 0.8 to 1.01  -  734 pairs\n",
      "0.04710390232503414,0.8822254240512848,0.9894763678312302\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_both_ud, nn_sim_both_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "08515ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  858 pairs\n",
      "0.998531699180603,0.999650627374649,0.9999016225337982\n",
      "For range 0.2 to 0.4  -  968 pairs\n",
      "0.9988743215799332,0.9996317327022552,0.999903067946434\n",
      "For range 0.4 to 0.6  -  581 pairs\n",
      "0.9990769028663635,0.9997349381446838,0.9999250173568726\n",
      "For range 0.6 to 0.8  -  749 pairs\n",
      "0.9989840388298035,0.999674916267395,0.9999105334281921\n",
      "For range 0.8 to 1.01  -  734 pairs\n",
      "0.998889610171318,0.999614417552948,0.9998851418495178\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_both_ud, novel_sim_both_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996ecac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
