{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3bc98ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ericm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ericm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries - Python 3.8\n",
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import sklearn.preprocessing as pp\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Concatenate, Input, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import one_hot\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "join = os.path.join\n",
    "\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries - Python 3.6\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "join = os.path.join\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2dc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284125b1",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb61888",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"./../data/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6696da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "\n",
    "video1_csv = \"baseline1_final.csv\"\n",
    "df_video1 = pd.read_csv(join(root_path, video1_csv), sep='\\t')\n",
    "\n",
    "channel_csv = \"baseline_final_channels.csv\"\n",
    "df_channel = pd.read_csv(join(root_path, channel_csv), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c250e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_video1.shape)\n",
    "df_video1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_channel.shape)\n",
    "df_channel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39874ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARDCODED limits per topic for baseline1 dataset\n",
    "TOPIC_LIMITS = [0, 430, 901, 1214, 1530, 2120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5aefebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions1_list = df_video1[\"caption\"].to_list()\n",
    "video1_list = df_video1[\"video_id\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92f4a5",
   "metadata": {},
   "source": [
    "### Captions -> word2vec, extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1304fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec\n",
    "word2vec_300 = \"GoogleNews-vectors-negative300.bin\"\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(join(root_path, word2vec_300), binary = True)\n",
    "word2vec_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe751bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    return np.mean(word2vec_model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our earlier preprocessing was done when we were dealing only with word vectors\n",
    "# Here, we need each document to remain a document \n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word.isalpha()] \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will help us drop documents that have no word vectors in word2vec\n",
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.vocab for word in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out documents\n",
    "def filter_docs(corpus, texts, video_ids, condition_on_doc):\n",
    "    \"\"\"\n",
    "    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "\n",
    "    ret_texts, ret_videos = [], []\n",
    "    if texts is not None:\n",
    "        for (text, doc, video_id) in zip(texts, corpus, video_ids):\n",
    "            if condition_on_doc(doc):\n",
    "                ret_texts.append(text)\n",
    "                ret_videos.append(video_id)\n",
    "\n",
    "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
    "\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, ret_texts, ret_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def captions_to_word2vec(caption_list, video_list, model):\n",
    "    # Preprocess the corpus\n",
    "    corpus = [preprocess(title) for title in caption_list]\n",
    "\n",
    "    # Remove docs that don't include any words in W2V's vocab\n",
    "    corpus, titles_list, video_list = filter_docs(corpus, caption_list, video_list, lambda doc: has_vector_representation(model, doc))\n",
    "\n",
    "    # Filter out any empty docs\n",
    "    corpus, titles_list, video_list = filter_docs(corpus, caption_list, video_list, lambda doc: (len(doc) != 0))\n",
    "    x = []\n",
    "    for doc in corpus: # append the vector for each document\n",
    "        x.append(document_vector(model, doc))\n",
    "\n",
    "    X = np.array(x) # list to array\n",
    "    \n",
    "    return X, video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccda93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions1_X, cap_w2v_videos = captions_to_word2vec(captions1_list, video1_list, word2vec_model)\n",
    "captions1_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77797ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(join(root_path, 'baseline1_word2vec_300.npy'), 'wb') as f:\n",
    "    np.save(f, captions1_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "761dfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "assert os.path.isfile(join(root_path, 'baseline1_word2vec_300.npy'))\n",
    "captions1_X = np.load(join(root_path, 'baseline1_word2vec_300.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513815b",
   "metadata": {},
   "source": [
    "### Captions -> fastText\n",
    "Source: https://github.com/kostantinos-papadamou/pseudoscience-paper\n",
    "Use Python 3.6 to run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat data, save to file\n",
    "with open(join(root_path, 'fasttext_captions.txt'), 'w') as f:\n",
    "    for item in captions1_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune fasttext model, save to file\n",
    "fasttext_models_filename = 'fasttext_model_finetuned.bin'\n",
    "ft_model = fasttext.train_unsupervised(\n",
    "    input=join(root_path, 'fasttext_captions.txt'),\n",
    "    pretrainedVectors=join(root_path, 'wiki-news-300d-1M.vec'),\n",
    "    dim=300,\n",
    "    minn=2,\n",
    "    maxn=5,\n",
    "    verbose=2)\n",
    "ft_model.save_model(join(root_path, fasttext_models_filename))\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get caption features\n",
    "ft_model = fasttext.load_model(join(root_path, \"fasttext_model_finetuned.bin\"))\n",
    "ft_features = []\n",
    "for caption in captions1_list:\n",
    "    ft_features += [ft_model.get_sentence_vector(text=caption)]\n",
    "ft_features = np.array(ft_features)\n",
    "print(ft_features.shape)\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79da6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(join(root_path, 'baseline1_fasttext_300.npy'), 'wb') as f:\n",
    "    np.save(f, ft_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc798ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "assert os.path.isfile(join(root_path, 'baseline1_fasttext_300.npy'))\n",
    "ft_features = np.load(join(root_path, 'baseline1_fasttext_300.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f8765e",
   "metadata": {},
   "source": [
    "### Train deep learning model on fastText, extract embeddings\n",
    "Source: https://github.com/kostantinos-papadamou/pseudoscience-paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b4222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify GPU environment\n",
    "gpu_training = True\n",
    "if gpu_training:\n",
    "    # Train on GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "else:\n",
    "    # Train on CPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d32559d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve labels\n",
    "ft_labels = df_video1[\"label\"].to_numpy()\n",
    "ft_labels[ft_labels == -1] = 0  # make two class\n",
    "ft_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79da9349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " fully_connected_1 (Dense)   (None, 256)               77056     \n",
      "                                                                 \n",
      " dropout_layer_1 (Dropout)   (None, 256)               0         \n",
      "                                                                 \n",
      " fully_connected_2 (Dense)   (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_layer_2 (Dropout)   (None, 128)               0         \n",
      "                                                                 \n",
      " fully_connected_3 (Dense)   (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_layer_3 (Dropout)   (None, 64)                0         \n",
      "                                                                 \n",
      " fully_connected_4 (Dense)   (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_layer_4 (Dropout)   (None, 32)                0         \n",
      "                                                                 \n",
      " classification_layer (Dense  (None, 2)                66        \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,354\n",
      "Trainable params: 120,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ericm/anaconda3/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "class PsuedoscienceDeepLearningModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize hyperparameters\n",
    "        self.embedding_dim = 300\n",
    "        self.dropout = 0.5\n",
    "        self.learning_rate = 1e-3\n",
    "        self.val_split_size = 0.2\n",
    "        self.num_epochs = 100\n",
    "        self.num_folds = 10  # for k-fold cross validation\n",
    "        self.batch_size = 20\n",
    "        self.shuffle_train_set = True\n",
    "        self.oversampling = True\n",
    "        self.num_classes = 2\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        seq = Sequential()\n",
    "        seq.add(Dense(units=256, activation='relu', name='fully_connected_1', input_shape=(self.embedding_dim,)))\n",
    "        seq.add(Dropout(rate=self.dropout, name='dropout_layer_1'))\n",
    "        seq.add(Dense(units=128, activation='relu', name='fully_connected_2'))\n",
    "        seq.add(Dropout(rate=self.dropout, name='dropout_layer_2'))\n",
    "        seq.add(Dense(units=64, activation='relu', name='fully_connected_3'))\n",
    "        seq.add(Dropout(rate=self.dropout, name='dropout_layer_3'))\n",
    "        seq.add(Dense(units=32, activation='relu', name='fully_connected_4'))\n",
    "        seq.add(Dropout(rate=self.dropout, name='dropout_layer_4'))\n",
    "        seq.add(Dense(units=self.num_classes, activation='softmax', name='classification_layer'))\n",
    "        seq.compile(loss=BinaryCrossentropy(from_logits=False),\n",
    "                    optimizer=Adam(lr=self.learning_rate),\n",
    "                    metrics=[F1Score(num_classes=2)])\n",
    "        return seq\n",
    "    \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "model = PsuedoscienceDeepLearningModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc79476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               verbose=2,\n",
    "                               mode='auto',\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "796ea3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n---Training the Model with 2120 videos.\n",
      "--- [AFTER OVER-SAMPLING] TRAIN: 2994\n",
      "Epoch 1/100\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.6955 - f1_score: 0.5040 - val_loss: 0.7048 - val_f1_score: 0.1055\n",
      "Epoch 2/100\n",
      "149/150 [============================>.] - ETA: 0s - loss: 0.6943 - f1_score: 0.4908Restoring model weights from the end of the best epoch: 2.\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.6943 - f1_score: 0.4909 - val_loss: 0.6893 - val_f1_score: 0.4687\n",
      "Epoch 00002: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print('/n---Training the Model with {} videos.'.format(ft_features.shape[0]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ft_features, ft_labels, test_size=model.val_split_size)\n",
    "\n",
    "# Oversampling\n",
    "if model.oversampling:\n",
    "    smote = SMOTE(sampling_strategy='not majority')\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    print('--- [AFTER OVER-SAMPLING] TRAIN: %d' % (y_train.shape[0]))\n",
    "\n",
    "# Convert labels to 1-hot\n",
    "y_train = one_hot(y_train, model.num_classes)\n",
    "y_test = one_hot(y_test, model.num_classes)\n",
    "\n",
    "# Train the model\n",
    "model_train_input = [X_train]\n",
    "model_val_input = [X_test]\n",
    "m = model.get_model()\n",
    "m.fit(model_train_input,\n",
    "      y_train,\n",
    "      epochs=model.num_epochs,\n",
    "      batch_size=model.batch_size,\n",
    "      validation_data=[model_val_input, y_test],\n",
    "      shuffle=model.shuffle_train_set,\n",
    "      verbose=1,\n",
    "      callbacks=[early_stopping])\n",
    "\n",
    "# Save trained model\n",
    "m.save(join(root_path, 'nn.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c372305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = load_model(join(root_path, \"nn_75.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c125a4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve layer outputs\n",
    "layer_name = 'fully_connected_4'\n",
    "intermediate_model = Model(inputs=best_model.input,\n",
    "                          outputs=best_model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_model.predict(ft_features)\n",
    "intermediate_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d911e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(join(root_path, 'baseline1_nn_32.npy'), 'wb') as f:\n",
    "    np.save(f, intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97559605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "assert os.path.isfile(join(root_path, 'baseline1_nn_32.npy'))\n",
    "intermediate_output = np.load(join(root_path, 'baseline1_nn_32.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9f1f9",
   "metadata": {},
   "source": [
    "### Load embeddings from our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4d9aba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = np.load(join(root_path, 'caption_embedding.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "758244e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 384)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert from ragged tensor to normal numpy array\n",
    "X_novel = np.zeros((ce.shape[0], ce[0][0].shape[0]))\n",
    "for i in range(ce.shape[0]):\n",
    "    X_novel[i, :] = ce[i][0]\n",
    "X_novel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "41e52425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02485627,  0.02774838, -0.04501574],\n",
       "       [ 0.04443581,  0.01227851,  0.08846989],\n",
       "       [-0.10445649,  0.03703442,  0.06822192]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_novel[:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ee9c9",
   "metadata": {},
   "source": [
    "### Compute similarity between channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75e0e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarities(mat):\n",
    "    col_normed_mat = pp.normalize(coo_matrix(mat.T).tocsc(), axis=0)\n",
    "    return col_normed_mat.T * col_normed_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f10be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ground_truth_diff_for_channels(baseline_final, baseline_channels):\n",
    "    \n",
    "    num_channels = len(baseline_channels)\n",
    "    channel_ground_truth_proportion = np.empty(num_channels, dtype='float32')\n",
    "\n",
    "    for index, row in baseline_channels.iterrows():\n",
    "        videos = row['video_ids'].split(',')\n",
    "\n",
    "        channel_videos = baseline_final[baseline_final[\"video_id\"].isin(videos)]\n",
    "        proportion = np.mean(channel_videos[\"label\"].to_numpy())\n",
    "        channel_ground_truth_proportion[index] = proportion\n",
    "    \n",
    "    return channel_ground_truth_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd97424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ground_truth_diff(p):\n",
    "    return np.abs(p[:, np.newaxis] - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4614c1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884, 884)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute ground truth proportion differences for channels\n",
    "misinfo_proportions = compute_ground_truth_diff_for_channels(df_video1, df_channel)\n",
    "proportion_diffs = compute_ground_truth_diff(misinfo_proportions)\n",
    "proportion_diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab3a91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_channel_embeddings(df_video, df_channel, video_embeddings):\n",
    "    \n",
    "    num_channels = len(df_channel)\n",
    "    embedding_size = video_embeddings.shape[1]\n",
    "    channel_embeddings = np.empty((num_channels, embedding_size), dtype='float32')\n",
    "    \n",
    "    for index, row in df_channel.iterrows():\n",
    "        videos = row['video_ids'].split(',')\n",
    "        \n",
    "        video_i = df_video.index[df_video[\"video_id\"].isin(videos)]\n",
    "        channel_embeddings[index, :] = np.mean(video_embeddings[video_i, :], axis=0)\n",
    "    \n",
    "    return channel_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0782351f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((884, 300), (884, 32), (884, 384))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute channel embeddings for baselines\n",
    "word2vec_channel_emb = compute_channel_embeddings(df_video1, df_channel, captions1_X)\n",
    "nn_channel_emb = compute_channel_embeddings(df_video1, df_channel, intermediate_output)\n",
    "novel_channel_emb = compute_channel_embeddings(df_video1, df_channel, X_novel)\n",
    "word2vec_channel_emb.shape, nn_channel_emb.shape, novel_channel_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2be9d990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((884, 884), (884, 884), (884, 884))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cosine similarities\n",
    "word2vec_channel_sim = cosine_similarities(word2vec_channel_emb).toarray()\n",
    "nn_channel_sim = cosine_similarities(nn_channel_emb).toarray()\n",
    "novel_channel_sim = cosine_similarities(novel_channel_emb).toarray()\n",
    "word2vec_channel_sim.shape, nn_channel_sim.shape, novel_channel_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b2e349cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09198472 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# All cosine similarities must be between 0 and 1\n",
    "\n",
    "word2vec_channel_sim[word2vec_channel_sim > 1.0] = 1.0\n",
    "word2vec_channel_sim[word2vec_channel_sim < 0.0] = 0.0\n",
    "nn_channel_sim[nn_channel_sim > 1.0] = 1.0\n",
    "nn_channel_sim[nn_channel_sim < 0.0] = 0.0\n",
    "novel_channel_sim[novel_channel_sim > 1.0] = 1.0\n",
    "novel_channel_sim[novel_channel_sim < 0.0] = 0.0\n",
    "\n",
    "print(word2vec_channel_sim.min(), word2vec_channel_sim.max())\n",
    "print(nn_channel_sim.min(), nn_channel_sim.max())\n",
    "print(novel_channel_sim.min(), novel_channel_sim.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0f8d9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 37, 125)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter channels according to number of videos\n",
    "# A minimum of 3 is required for fair comparison\n",
    "filtered_indices = []\n",
    "filtered_misinfo_indices = []\n",
    "filtered_nonmisinfo_indices = []\n",
    "for index, row in df_channel.iterrows():\n",
    "    videos = row['video_ids'].split(',')\n",
    "    if len(videos) >= 3:\n",
    "        filtered_indices += [index]\n",
    "        \n",
    "        channel_videos = df_video1[df_video1[\"video_id\"].isin(videos)]\n",
    "        if (channel_videos['label'] == 1).sum() > 0:\n",
    "            filtered_misinfo_indices += [index]\n",
    "        else:\n",
    "            filtered_nonmisinfo_indices += [index]\n",
    "\n",
    "len(filtered_indices), len(filtered_misinfo_indices), len(filtered_nonmisinfo_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f346e0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((162, 162), (37, 37), (125, 125))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter proportion diff and similarity matrices accordingly\n",
    "pd_all = proportion_diffs[filtered_indices, :][:, filtered_indices]\n",
    "pd_misinfo = proportion_diffs[filtered_misinfo_indices, :][:, filtered_misinfo_indices]\n",
    "pd_nonmisinfo = proportion_diffs[filtered_nonmisinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "w2v_sim_all = word2vec_channel_sim[filtered_indices, :][:, filtered_indices]\n",
    "w2v_sim_misinfo = word2vec_channel_sim[filtered_misinfo_indices, :][:, filtered_misinfo_indices]\n",
    "w2v_sim_nonmisinfo = word2vec_channel_sim[filtered_nonmisinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "nn_sim_all = nn_channel_sim[filtered_indices, :][:, filtered_indices]\n",
    "nn_sim_misinfo = nn_channel_sim[filtered_misinfo_indices, :][:, filtered_misinfo_indices]\n",
    "nn_sim_nonmisinfo = nn_channel_sim[filtered_nonmisinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "novel_sim_all = novel_channel_sim[filtered_indices, :][:, filtered_indices]\n",
    "novel_sim_misinfo = novel_channel_sim[filtered_misinfo_indices, :][:, filtered_misinfo_indices]\n",
    "novel_sim_nonmisinfo = novel_channel_sim[filtered_nonmisinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "pd_all.shape, pd_misinfo.shape, pd_nonmisinfo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a6c22d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13041,), (666,), (7750,))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract upper diagonals as flat matrix\n",
    "\n",
    "ind_all = np.triu_indices(len(filtered_indices), k=1)\n",
    "ind_misinfo = np.triu_indices(len(filtered_misinfo_indices), k=1)\n",
    "ind_nonmisinfo = np.triu_indices(len(filtered_nonmisinfo_indices), k=1)\n",
    "\n",
    "pd_all_ud = pd_all[ind_all]\n",
    "pd_misinfo_ud = pd_misinfo[ind_misinfo]\n",
    "pd_nonmisinfo_ud = pd_nonmisinfo[ind_nonmisinfo]\n",
    "\n",
    "w2v_sim_all_ud = w2v_sim_all[ind_all]\n",
    "w2v_sim_misinfo_ud = w2v_sim_misinfo[ind_misinfo]\n",
    "w2v_sim_nonmisinfo_ud = w2v_sim_nonmisinfo[ind_nonmisinfo]\n",
    "\n",
    "nn_sim_all_ud = nn_sim_all[ind_all]\n",
    "nn_sim_misinfo_ud = nn_sim_misinfo[ind_misinfo]\n",
    "nn_sim_nonmisinfo_ud = nn_sim_nonmisinfo[ind_nonmisinfo]\n",
    "\n",
    "novel_sim_all_ud = novel_sim_all[ind_all]\n",
    "novel_sim_misinfo_ud = novel_sim_misinfo[ind_misinfo]\n",
    "novel_sim_nonmisinfo_ud = novel_sim_nonmisinfo[ind_nonmisinfo]\n",
    "\n",
    "pd_all_ud.shape, pd_misinfo_ud.shape, pd_nonmisinfo_ud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "16c5c3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4625,), (4625,), (4625,), (4625,))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract cosine similarities of (misinfo, nonmisinfo) pairs\n",
    "# This needs to be done separately as it won't be a square matrix\n",
    "\n",
    "pd_both = proportion_diffs[filtered_misinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "w2v_sim_both = word2vec_channel_sim[filtered_misinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "nn_sim_both = nn_channel_sim[filtered_misinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "novel_sim_both = novel_channel_sim[filtered_misinfo_indices, :][:, filtered_nonmisinfo_indices]\n",
    "\n",
    "# Naming convention: _ud to be similar to other variables\n",
    "# No actual upper triangular calculation here\n",
    "pd_both_ud = pd_both.flatten()\n",
    "w2v_sim_both_ud = w2v_sim_both.flatten()\n",
    "nn_sim_both_ud = nn_sim_both.flatten()\n",
    "novel_sim_both_ud = novel_sim_both.flatten()\n",
    "\n",
    "pd_both_ud.shape, w2v_sim_both_ud.shape, nn_sim_both_ud.shape, novel_sim_both_ud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "004423b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_analysis_by_range(proportion_diffs_ud, sim_ud):\n",
    "\n",
    "    ranges = [(0.0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.01)]\n",
    "    for begin, end in ranges:\n",
    "\n",
    "        ind = np.argwhere((proportion_diffs_ud >= begin) & (proportion_diffs_ud < end))\n",
    "        \n",
    "        if ind.shape[0] > 0:\n",
    "            #proportion_diffs_i = proportion_diffs_ud[ind]\n",
    "            sim_i = sim_ud[ind]\n",
    "\n",
    "            print(\"For range\", begin, \"to\", end, \" - \", sim_i.shape[0], \"pairs\")\n",
    "            #print(\"25th percentile =\", np.percentile(sim_i, 25))\n",
    "            #print(\"50th percentile =\", np.percentile(sim_i, 50))\n",
    "            #print(\"75th percentile =\", np.percentile(sim_i, 75))\n",
    "            print(\"{0},{1},{2}\".format(np.percentile(sim_i, 25), np.percentile(sim_i, 50), np.percentile(sim_i, 75)))\n",
    "        else:\n",
    "            print(\"No pairs for range\", begin, \"to\", end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4a01d",
   "metadata": {},
   "source": [
    "#### Results: any label to any label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d4a8bc29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  9133 pairs\n",
      "0.9259091019630432,0.9569250345230103,0.9747517108917236\n",
      "For range 0.2 to 0.4  -  1750 pairs\n",
      "0.9337780624628067,0.9588366746902466,0.973519578576088\n",
      "For range 0.4 to 0.6  -  271 pairs\n",
      "0.9456446170806885,0.9669362306594849,0.9810445010662079\n",
      "For range 0.6 to 0.8  -  1214 pairs\n",
      "0.9366919100284576,0.9611344337463379,0.9744926393032074\n",
      "For range 0.8 to 1.01  -  673 pairs\n",
      "0.9479347467422485,0.9659979343414307,0.9776875376701355\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_all_ud, w2v_sim_all_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1e9c0ddb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  9133 pairs\n",
      "0.9773192405700684,0.9969475269317627,0.9994640350341797\n",
      "For range 0.2 to 0.4  -  1750 pairs\n",
      "0.5526943355798721,0.991633415222168,0.9985706955194473\n",
      "For range 0.4 to 0.6  -  271 pairs\n",
      "0.5335564315319061,0.5732368230819702,0.9283021092414856\n",
      "For range 0.6 to 0.8  -  1214 pairs\n",
      "0.07796964980661869,0.9157878458499908,0.9894870817661285\n",
      "For range 0.8 to 1.01  -  673 pairs\n",
      "0.008336368948221207,0.05354585871100426,0.3391764461994171\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_all_ud, nn_sim_all_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5e579aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  9133 pairs\n",
      "0.22426140308380127,0.3255103826522827,0.4226641356945038\n",
      "For range 0.2 to 0.4  -  1750 pairs\n",
      "0.22420957311987877,0.31792305409908295,0.4132193326950073\n",
      "For range 0.4 to 0.6  -  271 pairs\n",
      "0.2526658922433853,0.3467214107513428,0.4345874637365341\n",
      "For range 0.6 to 0.8  -  1214 pairs\n",
      "0.21507947891950607,0.30437514185905457,0.4053845852613449\n",
      "For range 0.8 to 1.01  -  673 pairs\n",
      "0.24716150760650635,0.3396175503730774,0.4340020418167114\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_all_ud, novel_sim_all_ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a3992",
   "metadata": {},
   "source": [
    "#### Results: misinfo to misinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "801160b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  258 pairs\n",
      "0.9506474882364273,0.9678962230682373,0.9787565022706985\n",
      "For range 0.2 to 0.4  -  125 pairs\n",
      "0.9571177363395691,0.9734190702438354,0.9796338677406311\n",
      "For range 0.4 to 0.6  -  146 pairs\n",
      "0.9452119767665863,0.9641998410224915,0.9775935262441635\n",
      "For range 0.6 to 0.8  -  89 pairs\n",
      "0.9602387547492981,0.971376895904541,0.9801899194717407\n",
      "For range 0.8 to 1.01  -  48 pairs\n",
      "0.9673985242843628,0.9770538508892059,0.9858627766370773\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_misinfo_ud, w2v_sim_misinfo_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "39c1ba40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  258 pairs\n",
      "0.2811216786503792,0.9841984510421753,0.9979343265295029\n",
      "For range 0.2 to 0.4  -  125 pairs\n",
      "0.4025932252407074,0.9171246886253357,0.9972336292266846\n",
      "For range 0.4 to 0.6  -  146 pairs\n",
      "0.13459418341517448,0.9181327819824219,0.9896990805864334\n",
      "For range 0.6 to 0.8  -  89 pairs\n",
      "0.0458090677857399,0.36094900965690613,0.9177894592285156\n",
      "For range 0.8 to 1.01  -  48 pairs\n",
      "0.02807043818756938,0.058897823095321655,0.10355405882000923\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_misinfo_ud, nn_sim_misinfo_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "530dc646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  258 pairs\n",
      "0.2979564368724823,0.3913784772157669,0.5138677060604095\n",
      "For range 0.2 to 0.4  -  125 pairs\n",
      "0.27309828996658325,0.37327659130096436,0.44910871982574463\n",
      "For range 0.4 to 0.6  -  146 pairs\n",
      "0.25673896074295044,0.3468211442232132,0.42464208602905273\n",
      "For range 0.6 to 0.8  -  89 pairs\n",
      "0.33741623163223267,0.39947211742401123,0.520767867565155\n",
      "For range 0.8 to 1.01  -  48 pairs\n",
      "0.3940044492483139,0.45766226947307587,0.5265971124172211\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_misinfo_ud, novel_sim_misinfo_ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89d41f",
   "metadata": {},
   "source": [
    "#### Results: nonmisinfo to nonmisinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1a9e0b78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  7750 pairs\n",
      "0.9217846244573593,0.9540671706199646,0.9731397479772568\n",
      "No pairs for range 0.2 to 0.4\n",
      "No pairs for range 0.4 to 0.6\n",
      "No pairs for range 0.6 to 0.8\n",
      "No pairs for range 0.8 to 1.01\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_nonmisinfo_ud, w2v_sim_nonmisinfo_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5e9382b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  7750 pairs\n",
      "0.976519450545311,0.9969258010387421,0.9995447993278503\n",
      "No pairs for range 0.2 to 0.4\n",
      "No pairs for range 0.4 to 0.6\n",
      "No pairs for range 0.6 to 0.8\n",
      "No pairs for range 0.8 to 1.01\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_nonmisinfo_ud, nn_sim_nonmisinfo_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "31816cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  7750 pairs\n",
      "0.21172930300235748,0.3105342984199524,0.40668080002069473\n",
      "No pairs for range 0.2 to 0.4\n",
      "No pairs for range 0.4 to 0.6\n",
      "No pairs for range 0.6 to 0.8\n",
      "No pairs for range 0.8 to 1.01\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_nonmisinfo_ud, novel_sim_nonmisinfo_ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1d0be",
   "metadata": {},
   "source": [
    "#### Results: misinfo to nonmisinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "416668c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  1125 pairs\n",
      "0.9460095763206482,0.9685550332069397,0.9811440110206604\n",
      "For range 0.2 to 0.4  -  1625 pairs\n",
      "0.9324469566345215,0.9578205943107605,0.9724388122558594\n",
      "For range 0.4 to 0.6  -  125 pairs\n",
      "0.9456844329833984,0.9709511399269104,0.9843785762786865\n",
      "For range 0.6 to 0.8  -  1125 pairs\n",
      "0.9349206686019897,0.960141122341156,0.9735148549079895\n",
      "For range 0.8 to 1.01  -  625 pairs\n",
      "0.9469743967056274,0.9641695618629456,0.9771322011947632\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_both_ud, w2v_sim_both_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "728f3611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  1125 pairs\n",
      "0.991191565990448,0.9977689385414124,0.9992472529411316\n",
      "For range 0.2 to 0.4  -  1625 pairs\n",
      "0.6346403360366821,0.9918893575668335,0.9986367225646973\n",
      "For range 0.4 to 0.6  -  125 pairs\n",
      "0.5360397696495056,0.5589452981948853,0.6129161715507507\n",
      "For range 0.6 to 0.8  -  1125 pairs\n",
      "0.08983024209737778,0.9198633432388306,0.9897669553756714\n",
      "For range 0.8 to 1.01  -  625 pairs\n",
      "0.008286419324576855,0.052968502044677734,0.3504315912723541\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_both_ud, nn_sim_both_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "08515ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For range 0.0 to 0.2  -  1125 pairs\n",
      "0.32376864552497864,0.40717804431915283,0.4961298704147339\n",
      "For range 0.2 to 0.4  -  1625 pairs\n",
      "0.22080214321613312,0.31561392545700073,0.40804314613342285\n",
      "For range 0.4 to 0.6  -  125 pairs\n",
      "0.2504729628562927,0.3467214107513428,0.43854254484176636\n",
      "For range 0.6 to 0.8  -  1125 pairs\n",
      "0.21093253791332245,0.2963474988937378,0.39278215169906616\n",
      "For range 0.8 to 1.01  -  625 pairs\n",
      "0.23968414962291718,0.3311050534248352,0.426180362701416\n"
     ]
    }
   ],
   "source": [
    "result_analysis_by_range(pd_both_ud, novel_sim_both_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996ecac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
